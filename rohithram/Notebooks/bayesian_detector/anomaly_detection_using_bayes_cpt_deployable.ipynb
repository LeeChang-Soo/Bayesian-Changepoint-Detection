{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Online Changepoint Detection\n",
    "* It is a probabilistic model to identify the level shifts(mean-shifts) or variational shifts in incoming dataset\n",
    "* It predicts posterior probability of run length(interval between to changepoints in dataset) so for a dataset of n-samples, there are n-possible run lengths.\n",
    "* The model calculates probability that all those possible run lengths for each datapoint to be changepoints so it creates $NxN$ matrix where $N$ is length of the dataset\n",
    "* So we set default threshold probability for it be a changepoint is $0.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Arguments to run the program:\n",
    "\n",
    "* All of the algorithm related parameters are optional which has a default value given in the program itself.\n",
    "* Description for each argument is given below :\n",
    "\n",
    "* algo_kwargs={\n",
    "            'data_col_index':1  -> column index from which metrics to be considered for anomaly detection ,\n",
    "            'pthres':thres_prob -> probability after which a point is considered as changepoint,\n",
    "            'Nw':samples_to_wait -> samples to wait before detecting changepoints,\n",
    "            'mean_runlen':expected_run_length -> average run length between two change points in dataset\n",
    "        }\n",
    "        \n",
    "* So the default value given are as follows:\n",
    "     * algo_kwargs={\n",
    "            'data_col_index':1,\n",
    "            'pthres':0.5,\n",
    "            'Nw':10,\n",
    "            'mean_runlen':100\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies :\n",
    "* Download https://github.com/hildensia/bayesian_changepoint_detection this and run **python setup.py install** to install the bayesian changepoint detection module.\n",
    "* Install **writefile_run** using pip , which is used to save the cell in a python file automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import writefile_run as writefile_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = '../../anomaly_detectors/bayesian_detector/bayeschangept_wrapper.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run $filename\n",
    "\n",
    "'''\n",
    "importing all the required header files\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "import datetime as dt\n",
    "import time\n",
    "import os\n",
    "\n",
    "from anomaly_detectors.utils.data_handler import  *\n",
    "from anomaly_detectors.utils.preprocessors import *\n",
    "from anomaly_detectors.utils.error_codes import error_codes\n",
    "from anomaly_detectors.utils import type_checker as type_checker\n",
    "from anomaly_detectors.utils import csv_prep_for_reader as csv_helper\n",
    "from anomaly_detectors.utils import make_ackg_json\n",
    "from anomaly_detectors.bayesian_detector import bayesian_changept_detector\n",
    "\n",
    "import json\n",
    "import traceback\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 9\n",
    "rcParams[ 'axes.grid']=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "% matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile_run $filename -a\n",
    "\n",
    "'''\n",
    "ideal argument types for algorithm\n",
    "'''\n",
    "algo_params_type ={\n",
    "            'data_col_index':int,\n",
    "            'pthres':float or int,\n",
    "            'Nw':int,\n",
    "            'mean_runlen':int,\n",
    "            'to_plot':bool\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile_run $filename -a\n",
    "\n",
    "\n",
    "def main(json_data,thres_prob=0.5,samples_to_wait=10,expected_run_length=100,to_plot=True):\n",
    "\n",
    "        '''\n",
    "        Wrapper function which should be called inorder to run the anomaly detection, it has four parts :\n",
    "        \n",
    "        *reader           - Class Data_reader defined in data_handler.py which takes in json string as input and parses json \n",
    "                            and gives list of dataframes, incase of any error it returns dictionary with error message\n",
    "        *preprocessor     - preprocessors are defined in preprocessors.py, which takes in data and gives out processed \n",
    "                            data\n",
    "        *anomaly detector - Class Bayesian_Changept_Detector defined in bayesian_changept_detector.py, which takes in\n",
    "                            data and algorithm parameters as argument and returns anomaly indexes and data.        \n",
    "        *make_ack_json    - Function to make acknowledge json\n",
    "        \n",
    "        \n",
    "        Arguments :\n",
    "        \n",
    "        Required Parameter:\n",
    "            reader_kwargs: the Json object in the format of the input json given from reader api\n",
    "        Optional Parameter:\n",
    "            thres_prob (Type : Float , between 0 and 1) It is the probability threshold after which points are considered as change points\n",
    "            Default: 0.5\n",
    "            samples_to_wait: Positive Integer representing the no of samples after which the run length probability will be calculated. It is one of the algorithm related parameter\n",
    "            Default: 10\n",
    "            expected_run_length: positive Integer that is average run length or no of samples between two change-points. This originates from modeling the interval between change-points with exponential distribution\n",
    "            Default : 100\n",
    "            to_plot : Boolean .Give True to see the plots of change-points detected and False if there is no need for plotting\n",
    "            Default : True\n",
    "        Note:\n",
    "        To run this, import this python file as module and call this function with required args and it will detect\n",
    "        anomalies and writes to the local database.\n",
    "        This algorithm is univariate, so each metric per asset is processed individually\n",
    "        '''\n",
    "        \n",
    "        \n",
    "\n",
    "        #algorithm arguments\n",
    "        algo_kwargs={\n",
    "            'data_col_index':1,\n",
    "            'pthres':thres_prob,\n",
    "            'Nw':samples_to_wait,\n",
    "            'mean_runlen':expected_run_length,\n",
    "            'to_plot':to_plot\n",
    "        }\n",
    "              \n",
    "        '''\n",
    "            #reseting the error_codes to avoid overwritting\n",
    "            #error_codes is a python file imported as error_codes which has error_codes dictionary mapping \n",
    "        '''\n",
    "        error_codes1 = error_codes()\n",
    "        \n",
    "        try: \n",
    "                       \n",
    "            \n",
    "            # type_checker is python file which has Type_checker class which checks given parameter types\n",
    "            checker = type_checker.Type_checker(kwargs=algo_kwargs,ideal_args_type=algo_params_type)\n",
    "            # res is None when no error raised, otherwise it stores the appropriate error message\n",
    "            res = checker.params_checker()\n",
    "            if(res!=None):\n",
    "                return json.dumps(res)\n",
    "            \n",
    "            \n",
    "            # instanstiating the reader class with reader arguments\n",
    "            data_reader = Data_reader(json_data=json_data)\n",
    "            #getting list of dataframes per asset if not empty\n",
    "            #otherwise gives string 'Empty Dataframe'\n",
    "            entire_data = data_reader.read()\n",
    "            writer_data = []\n",
    "            anomaly_detectors = []\n",
    "            \n",
    "            if((len(entire_data)!=0 and entire_data!=None and type(entire_data)!=dict)):\n",
    "\n",
    "                '''\n",
    "                looping over the data per assets and inside that looping over metrics per asset\n",
    "                * Instantiates anomaly detector class with algo args and metric index to detect on\n",
    "                * Stores the anomaly indexes and anomaly detector object to bulk write to db at once\n",
    "                '''\n",
    "\n",
    "                for i,data_per_asset in enumerate(entire_data):\n",
    "                    assetno = pd.unique(data_per_asset['assetno'])[0]\n",
    "                    data_per_asset[data_per_asset.columns[1:]] = normalise_standardise(data_per_asset[data_per_asset.columns[1:]])\n",
    "                    print(\"Overview of data : \\n{}\\n\".format(data_per_asset.head()))\n",
    "\n",
    "                    for data_col in range(1,len(data_per_asset.columns[1:])+1):\n",
    "                        algo_kwargs['data_col_index'] = data_col\n",
    "                        print(\"\\nAnomaly detection for AssetNo : {} ,Metric : {}\\n \".format(\n",
    "                            assetno,data_per_asset.columns[data_col]))\n",
    "                        \n",
    "                        \n",
    "                        anomaly_detector = bayesian_changept_detector.Bayesian_Changept_Detector(data_per_asset,\n",
    "                                                                                                 assetno=assetno,\n",
    "                                                                                                 **algo_kwargs)\n",
    "                        data,anom_indexes = anomaly_detector.detect_anomalies()\n",
    "\n",
    "                        anomaly_detectors.append(anomaly_detector)\n",
    "                        \n",
    "                ack_json = {}\n",
    "                ack_json = make_ackg_json.make_ack_json(anomaly_detectors)\n",
    "                        \n",
    "                return json.dumps(ack_json)\n",
    "            elif(type(entire_data)==dict):\n",
    "                return json.dumps(entire_data)\n",
    "            else:\n",
    "                '''\n",
    "                Data empty errors\n",
    "                '''\n",
    "                return json.dumps(error_codes1['data_missing'])\n",
    "        except Exception as e:\n",
    "            '''\n",
    "            unknown exceptions are caught here and traceback used to know the source of the error\n",
    "            '''\n",
    "            traceback.print_exc()\n",
    "            \n",
    "            error_codes1['unknown']['message']=str(e)\n",
    "            return json.dumps(error_codes1['unknown'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_kwargs = lambda :{\n",
    "            'filepath'   :'../../dataset/sample_csv_files/alcohol-demand-log-spirits-consu.csv',\n",
    "            'filename'   :'alcohol-demand-log-spirits-consu.csv',\n",
    "            'target_dir' :'../../dataset/reader_csv_files/',\n",
    "            'assetno'    :'A1',\n",
    "            'n_rows'     :None,\n",
    "            'has_time'   :True\n",
    "            }\n",
    "\n",
    "algo_kwargs =lambda: {\n",
    "            'thres_prob':0.5,\n",
    "            'samples_to_wait':10,\n",
    "            'expected_run_length':100,\n",
    "            'to_plot':True\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-9e897e5715c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mreader_kwargs1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0malgo_kwargs1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgo_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0malgo_kwargs1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json_data' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Dictionary of arguments given to wrapper function which executes this whole program for detecting changepoints \n",
    "and gives output json of anomaly timestamps\n",
    "'''\n",
    "\n",
    "reader_kwargs1 = reader_kwargs()\n",
    "algo_kwargs1 = algo_kwargs()\n",
    "res = main(json_data=json_data,**algo_kwargs1)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the algo on sample csv datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detecting anomalies for methane-input-into-gas-furnace-c.csv\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'reader_helper' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-a5eebca8c716>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                                                  assetno=assetno)\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mjson_data\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mreader_helper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader_kwargs1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0malgo_kwargs1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgo_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0malgo_kwargs1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'thres_prob'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reader_helper' is not defined"
     ]
    }
   ],
   "source": [
    "data_dir = '../../dataset/sample_csv_files/'\n",
    "target_dir = '../../dataset/reader_csv_files/'\n",
    "assetno = 'A1'\n",
    "for filename in os.listdir(data_dir):\n",
    "    \n",
    "    name,ext = os.path.splitext(filename)\n",
    "    if ext != '.csv':continue\n",
    "    infile = os.path.join(data_dir,filename)\n",
    "    print(\"\\nDetecting anomalies for {}\\n\".format(filename))\n",
    "    \n",
    "    con,params = csv_helper.preparecsvtoread(filepath=infile,filename=filename,target_dir=target_dir,\n",
    "                                                 assetno=assetno)\n",
    "    \n",
    "    json_data  = reader_helper.read(reader_kwargs1)\n",
    "    algo_kwargs1 = algo_kwargs()\n",
    "    algo_kwargs1['thres_prob'] = 0.4\n",
    "    res = main(**algo_kwargs1,json_data=json_data)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "* Hence we observe that **Bayesian Changepoint Detection** works well only on level shifts or variational shift datasets over outlier or surge,sag datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
