{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Handler\n",
    "* This python file handles both reading the data and writing data to database using separate classes for both cases.\n",
    "* Structure is : \n",
    "* Class **Data_reader** which contains methods which are used to fetch data using reader api from opentsdb as well as csv file.Which converts json response into list of dataframes per asset with multiple metric being columns from index $1$ onwards with timestamp in epoch being index of dataframe and assetno being the first column i.e column index $0$.\n",
    "* Class **Postgres_Writer** which is used for mapping the outputs of anomaly detector to corresponding ATL's and bulk writes the queries to local db at one shot.It is instantiated after anomaly detected for all metrics in an asset likewise for all such assets from master python file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import writefile_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../../anomaly_detectors/utils/data_handler.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run $filename\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "#importing reader and checker for reading data\n",
    "from anomaly_detectors.reader_writer import reader_new as reader\n",
    "from anomaly_detectors.reader_writer import checker as checker\n",
    "import datetime as dt\n",
    "# error code is python file which contains dictionary of mapped error codes and messages for different errors\n",
    "from anomaly_detectors.utils.error_codes import error_codes\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run $filename -a\n",
    "\n",
    "\n",
    "    \n",
    "class Postgres_Writer():\n",
    "    \n",
    "    \n",
    "    def __init__(self,anomaly_detectors,db_credentials,sql_query_args,table_name,window_size=10):\n",
    "        \n",
    "        '''\n",
    "        Used for mapping the outputs of anomaly detector to corresponding Asset timeline logging and bulk writes the \n",
    "        queries to local db at one shot.It is instantiated after anomaly detected for all metrics in \n",
    "        an asset likewise for all such assets from master python file.\n",
    "        Arguments need while instantiating this class are :\n",
    "        anomaly_detectors : List of anomaly detector objects (Its object of bayesian changept detector class)\n",
    "        db_credentials : dictionary of credentials for connecting to db\n",
    "        sql_query_args : dictionary of args or values used to form the columns in the table for each row\n",
    "        table_name : string - table name\n",
    "        window_size : (int) no of points either side of around anomaly to write into db\n",
    "        '''\n",
    "        \n",
    "        self.anomaly_detectors = anomaly_detectors\n",
    "        self.db_credentials = db_credentials\n",
    "        self.sql_query_args = sql_query_args\n",
    "        self.table_name = table_name\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        print(\"Postgres writer initialised \\n\")\n",
    "\n",
    "        \n",
    "    def write_to_db(self,col_names,col_vals):\n",
    "        \n",
    "        '''\n",
    "        connects to db and executes the query for bulk insert.\n",
    "        Arguments:\n",
    "        col_names : column names (comma separated)\n",
    "        col_vals  : list of column values for each row\n",
    "        '''\n",
    "        error_codes1 = error_codes()\n",
    "        \n",
    "        col_vals1 = [[str(val) if(type(val)!=str) else \"'{}'\".format(val) for val in row] for row in col_vals]\n",
    "        joined_col_vals = [\"({})\".format(','.join(map(str,val))) for val in col_vals1]\n",
    "        fmt_col_vals = (','.join(joined_col_vals))\n",
    "        insert_query = \"\"\" INSERT INTO {} ({}) VALUES{};\"\"\".format(self.table_name,col_names,fmt_col_vals)\n",
    "        \n",
    "        status = 0\n",
    "        conn = None\n",
    "        cur = None\n",
    "        try:\n",
    "            conn = psycopg2.connect(**self.db_credentials)\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(insert_query)\n",
    "            conn.commit()\n",
    "            cur.close()\n",
    "            conn.close()\n",
    "            print('\\n Successfully written into database\\n')\n",
    "            return error_codes1['success']\n",
    "        except psycopg2.DatabaseError as error:\n",
    "            status = 1\n",
    "            print(\"Database error : {}\".format(error))\n",
    "            error_codes1['db']['message']=str(error)\n",
    "            return error_codes1['db']\n",
    "        finally:\n",
    "                if cur is not None:\n",
    "                    cur.close()\n",
    "                if conn is not None:\n",
    "                    conn.close()\n",
    "        \n",
    "        \n",
    "    def ts_to_unix(self,t):\n",
    "        return int((t - dt.datetime(1970, 1, 1)).total_seconds()*1000)\n",
    "    \n",
    "    def map_outputs_and_write(self):\n",
    "        \n",
    "        '''\n",
    "        maps the values to corresponding columns of the table\n",
    "        Checks whether the algo is univariate or multivariate and proceeds accordingly\n",
    "        '''\n",
    "        \n",
    "        sql_query_args = self.sql_query_args\n",
    "        col_names_list = list(sql_query_args.keys())\n",
    "        col_names = ','.join(col_names_list)\n",
    "        col_vals = []\n",
    "        table_name = self.table_name\n",
    "        \n",
    "        if(self.anomaly_detectors[0].algo_type=='univariate'):\n",
    "            '''\n",
    "            # looping through the list of anomaly detectors which are instances for different metrics and assets\n",
    "            # They have info like anomaly indexes and data , etc\n",
    "            # appends the mapped column values to list for bulk inserting\n",
    "            '''\n",
    "            for anomaly_detector in self.anomaly_detectors:\n",
    "                \n",
    "                queries = self.make_query_args_univariate(anomaly_detector,sql_query_args)\n",
    "                [col_vals.append(list(query.values())) for query in queries]\n",
    "\n",
    "        else:\n",
    "            '''\n",
    "            in construction - will be updated shortly for multivariate case\n",
    "            '''\n",
    "            \n",
    "            print(\"\\nMultivariate writer initialised\")\n",
    "            \n",
    "            for anomaly_detector in self.anomaly_detectors:\n",
    "                assetno = anomaly_detector.assetno\n",
    "                sql_query_args = self.sql_query_args\n",
    "                queries = self.make_query_args_multivariate(anomaly_detector,sql_query_args)\n",
    "                \n",
    "                [col_vals.append(list(query.values())) for query in queries]\n",
    "        \n",
    "        # if there are nothing to write don't connect to db\n",
    "        if(len(col_vals)!=0):\n",
    "            return self.write_to_db(col_names,col_vals)\n",
    "        else:\n",
    "            print(\"\\nNo anomaly detected to write\\n\")\n",
    "            error_codes1 = error_codes()\n",
    "            return error_codes1['success']\n",
    "        \n",
    "        \n",
    "    def make_query_args_univariate(self,anomaly_detector,sql_query_args):\n",
    "        \n",
    "        '''\n",
    "        Function to map the details about an anomaly to columns present in log asset timeline table\n",
    "        Arguments :\n",
    "        Takes in single anomaly detector and loops over the anomalies and also sql query arguments.\n",
    "        Returns sql queries -> list of all mapped sql column values for each anomaly\n",
    "        '''\n",
    "        \n",
    "        sql_queries = []\n",
    "        \n",
    "        #Proceed only if no of anomalies detected are not zero\n",
    "        \n",
    "        if(anomaly_detector.anom_indexes is not None and len(anomaly_detector.anom_indexes)!=0):\n",
    "                \n",
    "                anom_indexes = anomaly_detector.anom_indexes\n",
    "                original_data = anomaly_detector.data\n",
    "                col_index = anomaly_detector.data_col_index\n",
    "                metric_name = original_data.columns[anomaly_detector.data_col_index]\n",
    "                assetno = anomaly_detector.assetno\n",
    "                window = self.window_size\n",
    "                \n",
    "                sql_query_args['event_name'] = '{}_'.format(original_data.columns[col_index])+anomaly_detector.algo_code+'_anomaly'\n",
    "                sql_query_args['event_source'] = anomaly_detector.algo_name\n",
    "                sql_query_args['operating_unit_serial_number'] = str(assetno)\n",
    "                sql_query_args['parameter_list'] = '[{}]'.format(original_data.columns[anomaly_detector.data_col_index])\n",
    "                \n",
    "                for i in anom_indexes:\n",
    "                    event_ctxt_info =  {\"body\":[]}\n",
    "                    data_per_asset = {\"asset\": '',\"readings\":[]}\n",
    "                    data_per_metric = {\"name\":'',\"datapoints\":''}\n",
    "\n",
    "                    time_series = (original_data.index[i-window:i+window])\n",
    "                    sql_query_args['event_timestamp'] =  str(pd.to_datetime(original_data.index[i],unit='ms',utc=True))\n",
    "                    sql_query_args['event_timestamp_epoch'] = str(int(original_data.index[i]))\n",
    "                    sql_query_args['created_date'] = str(pd.to_datetime(dt.datetime.now(),utc=True))\n",
    "                    time_around_anoms = [\"''{}''\".format((t)) for t in time_series]                    \n",
    "\n",
    "                    data_per_metric['name']=metric_name\n",
    "                    datapoints = (list(zip(time_around_anoms,list(original_data.iloc[i-window:i+window,col_index].values))))\n",
    "                    data_per_metric['datapoints'] = datapoints\n",
    "                    data_per_asset['asset'] = assetno\n",
    "                    data_per_asset['readings'].append(data_per_metric)\n",
    "                    event_ctxt_info['body'].append(data_per_asset)\n",
    "\n",
    "                    sql_query_args['event_context_info'] = json.dumps(event_ctxt_info)\n",
    "                    sql_queries.append(sql_query_args)\n",
    "\n",
    "        return (sql_queries)\n",
    "\n",
    "    def make_query_args_multivariate(self,anomaly_detector,sql_query_args):\n",
    "\n",
    "            '''\n",
    "            Function to map the details about an anomaly to columns present in log asset timeline table\n",
    "            Arguments :\n",
    "            Takes in single anomaly detector and loops over the anomalies and also sql query arguments.\n",
    "            Returns sql queries -> list of all mapped sql column values for each anomaly\n",
    "            '''\n",
    "\n",
    "            sql_queries = []\n",
    "\n",
    "            #Proceed only if no of anomalies detected are not zero\n",
    "\n",
    "            if(anomaly_detector.anom_indexes is not None and len(anomaly_detector.anom_indexes)!=0):\n",
    "\n",
    "                    anom_indexes = anomaly_detector.anom_indexes\n",
    "                    try:\n",
    "                        original_data = pd.DataFrame(anomaly_detector.data.numpy())\n",
    "                    except:\n",
    "                        original_data = anomaly_detector.data\n",
    "\n",
    "                    metric_names = anomaly_detector.metric_name\n",
    "                    assetno = anomaly_detector.assetno\n",
    "                    window = self.window_size\n",
    "\n",
    "                    sql_query_args['event_name'] = '{}_'.format('_'.join(metric_names))+anomaly_detector.algo_code+'_anomaly'\n",
    "                    sql_query_args['event_source'] = anomaly_detector.algo_name\n",
    "                    sql_query_args['operating_unit_serial_number'] = str(assetno)\n",
    "                    sql_query_args['parameter_list'] = '[{}]'.format(','.join(metric_names))\n",
    "\n",
    "                    for i in anom_indexes:\n",
    "\n",
    "                        time_series = (original_data.index[i-window:i+window])\n",
    "                        sql_query_args['event_timestamp'] =  str(pd.to_datetime(original_data.index[i],unit='ms',utc=True))\n",
    "                        sql_query_args['event_timestamp_epoch'] = str(int(original_data.index[i]))\n",
    "                        sql_query_args['created_date'] = str(pd.to_datetime(dt.datetime.now(),utc=True))\n",
    "                        time_around_anoms = [\"''{}''\".format((t)) for t in time_series]    \n",
    "\n",
    "                        event_ctxt_info =  {\"body\":[]}\n",
    "                        data_per_asset = {\"asset\": '',\"readings\":[]}\n",
    "                        data_per_asset['asset'] = assetno\n",
    "\n",
    "                        for k,metric_name in enumerate(metric_names):\n",
    "                            datapoints = (list(zip(time_around_anoms,\n",
    "                                                   list(original_data.iloc[i-window:i+window,1+k].values))))\n",
    "                            \n",
    "                            data_per_metric = {\"name\":metric_name,\"datapoints\":datapoints}\n",
    "\n",
    "                            data_per_asset['readings'].append(data_per_metric)\n",
    "                            \n",
    "                        event_ctxt_info['body'].append(data_per_asset)\n",
    "                        sql_query_args['event_context_info'] = json.dumps(event_ctxt_info)\n",
    "                        sql_queries.append(sql_query_args)\n",
    "\n",
    "            return (sql_queries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile_run $filename -a\n",
    "\n",
    "class Data_reader():\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Data_reader is a class which contains methods which are used to fetch data using reader api from opentsdb as \n",
    "    well as csv file.Which converts json response into list of dataframes per asset with multiple \n",
    "    metric being columns from index 1 onwards with timestamp in epoch being index \n",
    "    of dataframe and assetno being the first column i.e column index 0.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,json_data):\n",
    "        \n",
    "        #takes json data\n",
    "        self.json_data = json_data\n",
    "        print(\"Data reader initialised \\n\")\n",
    "\n",
    "    def read(self):\n",
    "        \n",
    "        try:\n",
    "            response_dict = json.loads(self.json_data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_codes1 = error_codes()\n",
    "            error_codes1['param']['message'] = '{},{}'.format(str(e),str(self.json_data))\n",
    "            return error_codes1['param']\n",
    "        \n",
    "\n",
    "        print(\"Getting the dataset from the reader....\\n\")\n",
    "        entire_data = self.parse_dict_to_dataframe(response_dict)\n",
    "        \n",
    "        return entire_data\n",
    "    \n",
    "    def parse_dict_to_dataframe(self,response_dict):\n",
    "        \n",
    "        '''\n",
    "        parses the json response from reader api to list of dataframes per asset and metrics being columns of\n",
    "        each of the dataframe with timestamps being the index and first column is assetno\n",
    "        Arguments: response json\n",
    "        Returns -> List of dataframes\n",
    "        '''\n",
    "        \n",
    "        entire_data_set = []\n",
    "        \n",
    "        if(len(response_dict['body'])!=0):\n",
    "            df = json_normalize(data=response_dict, record_path=['body', 'readings', 'datapoints'],\n",
    "                                meta=[['body', 'assetno'],\n",
    "               ['body', 'readings', 'name']])\n",
    "            df.columns = ['timestamp', 'values', 'assetno', 'parameters']\n",
    "            df = pd.pivot_table(df, values='values', index=['assetno','timestamp'], columns=['parameters'], aggfunc=np.mean)\n",
    "            data = df.reset_index(drop=False).rename_axis(None,axis=1)\n",
    "            \n",
    "            #making the index of the dataframe to be index and deleting the timestamp column\n",
    "            data.index = data['timestamp']\n",
    "            del data['timestamp']\n",
    "            \n",
    "            #separating the dataframe into groups of distinct assets\n",
    "            data_per_assets = data.groupby('assetno')\n",
    "            \n",
    "            #creating list of dataframes of different assetno and with all metrics being columns in each dataframe\n",
    "            for name,group in data_per_assets:\n",
    "                entire_data_set.append(group)\n",
    "                 \n",
    "        return entire_data_set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
